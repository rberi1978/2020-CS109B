{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS-109B Advanced Data Science\n",
    "## Lab 9: Recurrent Neural Networks (Part II)\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2020**<br>\n",
    "**Instructors:** Mark Glickman, Pavlos Protopapas, and Chris Tanner<br>\n",
    "**Lab Instructors:** Chris Tanner and Eleni Angelaki Kaxiras<br>\n",
    "**Content:** Srivatsan Srinivasan, Pavlos Protopapas, Chris Tanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.discussion {\n",
       "\tbackground-color: #ccffcc;\n",
       "\tborder-color: #88E97A;\n",
       "\tborder-left: 5px solid #0A8000; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "In this lab, we will continue where we left off in Lab 8. By the end of this lab, you should:\n",
    "- feel comfortable modelling sequences in `keras` via RNNs and its variants (GRUs, LSTMs)\n",
    "- have a good undertanding on how sequences -- any data that has some temporal semantics (e.g., time series, natural language, images etc.) -- fit into and benefit from a recurrent architecture\n",
    "- ask any other NLP questions that you're curious about. This lab is the closest one we'll ever have to being an Office Hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model: 231+432 = 665.... It's not ? Let's ask our LSTM\n",
    "\n",
    "In this exercise, we are going to teach addition to our model. Given two numbers (<999), the model outputs their sum (<9999). The input is provided as a string '231+432' and the model will provide its output as ' 663' (Here the empty space is the padding character). We are not going to use any external dataset and are going to construct our own dataset for this exercise.\n",
    "\n",
    "The exercise we attempt to do effectively \"translates\" a sequence of characters '231+432' to another sequence of characters ' 663' and hence, this class of models are called sequence-to-sequence models (aka seq2seq). Such architectures have profound applications in several real-life tasks such as machine translation, summarization, image captioning etc.\n",
    "\n",
    "To be clear, sequence-to-sequence (aka seq2seq) models take as input a sequence of length N and return a sequence of length M, where N and M may or may not differ, and every single observation/input may be of different values, too. For example, machine translation concerns converting text from one natural language to another (e.g., translating English to French). Google Translate is an example, and their system is a seq2seq model. The input (e.g., an English sentence) can be of any length, and the output (e.g., a French sentence) may be of any length.\n",
    "\n",
    "**Background knowledge:** The earliest and most simple seq2seq model works by having one RNN for the input, just like we've always done, and we refer to it as being an \"encoder.\" The final hidden state of the encoder RNN is fed as input to another RNN that we refer to as the \"decoder.\" The job of the decoder is to generate each token, one word at a time. This may seem really limiting, as it relies on the encoder encapsulating the entire input sequence with just 1 hidden layer. It seems unrealistic that we could encode an entire meaning of a sentence with just one hidden layer. Yet, results even in this simplistic manner can be quite impressive. In fact, these early results were compelling enough that these models immediately replaced the decades of earlier machine translation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, RepeatVector, TimeDistributed\n",
    "import numpy as np\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing and handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):        \n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    # converts a String of characters into a one-hot embedding/vector\n",
    "    def encode(self, C, num_rows):        \n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    # converts a one-hot embedding/vector into a String of characters\n",
    "    def decode(self, x, calc_argmax=True):        \n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 3\n",
    "MAXOUTPUTLEN = DIGITS + 1\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_digit():\n",
    "  return np.random.choice(list('0123456789'))  \n",
    "\n",
    "# generate a new number of length `DIGITS`\n",
    "def generate_number():\n",
    "  num_digits = np.random.randint(1, DIGITS + 1)  \n",
    "  return int(''.join( return_random_digit()\n",
    "                      for i in range(num_digits)))\n",
    "\n",
    "# generate `TRAINING_SIZE` # of pairs of random numbers\n",
    "def data_generate(num_examples):\n",
    "  questions = []\n",
    "  answers = []\n",
    "  seen = set()\n",
    "  print('Generating data...')\n",
    "  while len(questions) < TRAINING_SIZE:      \n",
    "      a, b = generate_number(), generate_number()\n",
    "        \n",
    "      # don't allow duplicates; this is good practice for training,\n",
    "      # as we will minimize memorizing seen examples\n",
    "      key = tuple(sorted((a, b)))\n",
    "      if key in seen:\n",
    "          continue\n",
    "      seen.add(key)\n",
    "    \n",
    "      # pad the data with spaces so that the length is always MAXLEN.\n",
    "      q = '{}+{}'.format(a, b)\n",
    "      query = q + ' ' * (MAXLEN - len(q))\n",
    "      ans = str(a + b)\n",
    "    \n",
    "      # answers can be of maximum size DIGITS + 1.\n",
    "      ans += ' ' * (MAXOUTPUTLEN - len(ans))\n",
    "      questions.append(query)\n",
    "      answers.append(ans)\n",
    "  print('Total addition questions:', len(questions))\n",
    "  return questions, answers\n",
    "\n",
    "def encode_examples(questions, answers):\n",
    "  x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.float32)\n",
    "  y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.float32)\n",
    "  for i, sentence in enumerate(questions):\n",
    "      x[i] = ctable.encode(sentence, MAXLEN)\n",
    "  for i, sentence in enumerate(answers):\n",
    "      y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "  indices = np.arange(len(y))\n",
    "  np.random.shuffle(indices)\n",
    "  return x[indices],y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 100000\n",
      "Training Data shape:\n",
      "X :  (90000, 7, 12)\n",
      "Y :  (90000, 4, 12)\n",
      "Sample Question(in encoded form) :\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "Sample Output :\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Sample Question(in decoded form) :  860+55  \n",
      "Sample Output :  915 \n"
     ]
    }
   ],
   "source": [
    "q, a = data_generate(TRAINING_SIZE)\n",
    "x, y = encode_examples(q,a)\n",
    "\n",
    "# divides our data into training and validation\n",
    "split_at = len(x) - len(x) // 10\n",
    "x_train, x_val, y_train, y_val = x[:split_at], x[split_at:],y[:split_at],y[split_at:]\n",
    "\n",
    "print('Training Data shape:')\n",
    "print('X : ', x_train.shape)\n",
    "print('Y : ', y_train.shape)\n",
    "\n",
    "print('Sample Question(in encoded form) :\\n', x_train[0], '\\nSample Output :\\n', y_train[0])\n",
    "print('Sample Question(in decoded form) : ', ctable.decode(x_train[0]),'\\nSample Output : ', ctable.decode(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's learn two wrapper functions in Keras - TimeDistributed and RepeatVector with some dummy examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``TimeDistributed`` is a wrapper function call that applies an input operation (e.g., NN layer) on **each** of the timesteps of an input data. [Read the documentation here](https://keras.io/layers/wrappers/). For instance, perhaps you have a 5-dimensional input. Instead of simply outputting a single value for each of the 5 inputs, let's say that you want to emit an 8-dimensional output for each input. We can do this simply by using a TimeDistrubted layer while specifying that we want a fully-connected layer that applies itself to each of the 5 input time steps and emits 8 outputs for each. Below, we illustrate such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input :  (1, 3, 5)\n",
      "Shape of output :  (1, 3, 8)\n"
     ]
    }
   ],
   "source": [
    "# Inputs will be a tensor of size: batch_size * time_steps * input_vector_dim(to Dense)\n",
    "# Output will be a tensor of size: batch_size * time_steps * output_vector_dim(i.e., 8)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Here, Dense() converts a 5-dim input vector to a 8-dim vector.\n",
    "model.add(TimeDistributed(Dense(8), input_shape=(3, 5)))\n",
    "input_array = np.random.randint(10, size=(1,3,5))\n",
    "print(\"Shape of input : \", input_array.shape)\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(\"Shape of output : \", output_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RepeatVector** repeats the vector a specified number of times. Dimension changes from batch_size * number of elements to batch_size* number of repetitions * number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 6)\n",
      "(None, 3, 6)\n",
      "Shape of input :  (1, 10)\n",
      "Shape of output :  (1, 3, 6)\n",
      "Input :  [373 693 770  97  71 363 102 507  16 594]\n",
      "Output :  [[-279.64703  342.97864  -31.02362 -472.6404   122.02716 -252.0235 ]\n",
      " [-279.64703  342.97864  -31.02362 -472.6404   122.02716 -252.0235 ]\n",
      " [-279.64703  342.97864  -31.02362 -472.6404   122.02716 -252.0235 ]]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# converts tensor from size of 1*10 to 1*6\n",
    "model.add(Dense(6, input_dim=10))\n",
    "print(model.output_shape)\n",
    "\n",
    "# converts tensor from size of 1*6 to size of 1*3*6\n",
    "model.add(RepeatVector(3))\n",
    "print(model.output_shape) \n",
    "\n",
    "input_array = np.random.randint(1000, size=(1, 10))\n",
    "print(\"Shape of input : \", input_array.shape)\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "\n",
    "print(\"Shape of output : \", output_array.shape)\n",
    "# note: `None` is the batch dimension\n",
    "print('Input : ', input_array[0])\n",
    "print('Output : ', output_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL ARCHITECTURE\n",
    "\n",
    "<img src=\"files/fig/LSTM_addition.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Whenever you are initializing a LSTM in Keras, by the default the option `return_sequences = False`. This means that at the end of the step the next component will only get to see the final hidden layer's values. On the other hand, if you set `return_sequences = True`, the LSTM component will return the hidden layer at each time step. It means that the next component should be able to consume inputs in that form. \n",
    "\n",
    "Think how this statement is relevant in terms of this model architecture and the TimeDistributed module we just learned.\n",
    "\n",
    "Build an encoder and decoder both single layer 128 nodes and an appropriate dense layer as needed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               275456    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4, 256)            525312    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 4, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 4, 12)             3084      \n",
      "=================================================================\n",
      "Total params: 1,329,164\n",
      "Trainable params: 1,329,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperaparams\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 2\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# ENCODING\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "model.add(RepeatVector(MAXOUTPUTLEN))\n",
    "\n",
    "# DECODING\n",
    "for _ in range(LAYERS):\n",
    "    # return hidden layer at each time step\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True)) \n",
    "\n",
    "model.add(TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well our model trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "90000/90000 [==============================] - 18s 202us/sample - loss: 1.7602 - accuracy: 0.3503 - val_loss: 1.5259 - val_accuracy: 0.4123\n",
      "Epoch 2/20\n",
      "90000/90000 [==============================] - 12s 134us/sample - loss: 1.3743 - accuracy: 0.4760 - val_loss: 1.2802 - val_accuracy: 0.5147\n",
      "Epoch 3/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 1.1911 - accuracy: 0.5458 - val_loss: 1.0462 - val_accuracy: 0.5941\n",
      "Epoch 4/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.8567 - accuracy: 0.6733 - val_loss: 0.7741 - val_accuracy: 0.7020\n",
      "Epoch 5/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.7210 - accuracy: 0.7243 - val_loss: 0.6869 - val_accuracy: 0.7361\n",
      "Epoch 6/20\n",
      "90000/90000 [==============================] - 12s 131us/sample - loss: 0.6348 - accuracy: 0.7578 - val_loss: 0.4124 - val_accuracy: 0.8416\n",
      "Epoch 7/20\n",
      "90000/90000 [==============================] - 12s 131us/sample - loss: 0.1951 - accuracy: 0.9392 - val_loss: 0.1226 - val_accuracy: 0.9614\n",
      "Epoch 8/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.0754 - accuracy: 0.9788 - val_loss: 0.0456 - val_accuracy: 0.9877\n",
      "Epoch 9/20\n",
      "90000/90000 [==============================] - 12s 131us/sample - loss: 0.0516 - accuracy: 0.9846 - val_loss: 0.0650 - val_accuracy: 0.9818\n",
      "Epoch 10/20\n",
      "90000/90000 [==============================] - 12s 131us/sample - loss: 0.0401 - accuracy: 0.9879 - val_loss: 0.2612 - val_accuracy: 0.9074\n",
      "Epoch 11/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.0240 - accuracy: 0.9937 - val_loss: 0.0071 - val_accuracy: 0.9988\n",
      "Epoch 12/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.0136 - val_accuracy: 0.9962\n",
      "Epoch 13/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.0384 - accuracy: 0.9887 - val_loss: 0.1316 - val_accuracy: 0.9538\n",
      "Epoch 14/20\n",
      "90000/90000 [==============================] - 12s 131us/sample - loss: 0.0182 - accuracy: 0.9948 - val_loss: 0.0039 - val_accuracy: 0.9993\n",
      "Epoch 15/20\n",
      "90000/90000 [==============================] - 12s 131us/sample - loss: 0.0274 - accuracy: 0.9919 - val_loss: 0.0069 - val_accuracy: 0.9984\n",
      "Epoch 16/20\n",
      "90000/90000 [==============================] - 12s 132us/sample - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.0401 - val_accuracy: 0.9859\n",
      "Epoch 17/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.0273 - accuracy: 0.9912 - val_loss: 0.0076 - val_accuracy: 0.9977\n",
      "Epoch 18/20\n",
      "90000/90000 [==============================] - 12s 131us/sample - loss: 0.0160 - accuracy: 0.9950 - val_loss: 0.0024 - val_accuracy: 0.9997\n",
      "Epoch 19/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.0256 - accuracy: 0.9920 - val_loss: 0.0087 - val_accuracy: 0.9979\n",
      "Epoch 20/20\n",
      "90000/90000 [==============================] - 12s 130us/sample - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "Finished iteration  1\n",
      "Question 864+87  True 951  Guess 951  Good job\n",
      "Question 339+804 True 1143 Guess 1143 Good job\n",
      "Question 81+171  True 252  Guess 252  Good job\n",
      "Question 83+828  True 911  Guess 911  Good job\n",
      "Question 32+797  True 829  Guess 829  Good job\n",
      "Question 24+836  True 860  Guess 860  Good job\n",
      "Question 340+610 True 950  Guess 950  Good job\n",
      "Question 357+20  True 377  Guess 377  Good job\n",
      "Question 680+702 True 1382 Guess 1382 Good job\n",
      "Question 908+479 True 1387 Guess 1387 Good job\n",
      "Question 29+790  True 819  Guess 819  Good job\n",
      "Question 393+29  True 422  Guess 422  Good job\n",
      "Question 45+293  True 338  Guess 338  Good job\n",
      "Question 43+0    True 43   Guess 43   Good job\n",
      "Question 958+183 True 1141 Guess 1141 Good job\n",
      "Question 759+7   True 766  Guess 766  Good job\n",
      "Question 1+741   True 742  Guess 742  Good job\n",
      "Question 547+88  True 635  Guess 635  Good job\n",
      "Question 746+261 True 1007 Guess 1007 Good job\n",
      "Question 840+882 True 1722 Guess 1722 Good job\n",
      "The model scored  100.0  % in its test.\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 2):\n",
    "    print()  \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=20,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so\n",
    "    # we can visualize errors.\n",
    "    print('Finished iteration ', iteration)\n",
    "    numcorrect = 0\n",
    "    numtotal = 20\n",
    "    \n",
    "    for i in range(numtotal):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Question', q, end=' ')\n",
    "        print('True', correct, end=' ')\n",
    "        print('Guess', guess, end=' ')\n",
    "        if guess == correct :\n",
    "          print('Good job')\n",
    "          numcorrect += 1\n",
    "        else:\n",
    "          print('Fail')\n",
    "    print('The model scored ', numcorrect*100/numtotal,' % in its test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 9+5     True 14   Guess 15   Fail\n",
      "Question 36+14   True 50   Guess 40   Fail\n",
      "Question 73+527  True 600  Guess 500  Fail\n",
      "Question 93+707  True 800  Guess 700  Fail\n",
      "Question 644+355 True 999  Guess 199  Fail\n",
      "Question 37+9    True 46   Guess 47   Fail\n",
      "Question 37+9    True 46   Guess 47   Fail\n",
      "Question 93+707  True 800  Guess 700  Fail\n",
      "Question 73+527  True 600  Guess 500  Fail\n",
      "Question 9+5     True 14   Guess 15   Fail\n",
      "Question 491+9   True 500  Guess 400  Fail\n",
      "Question 9+5     True 14   Guess 15   Fail\n",
      "Question 274+725 True 999  Guess 199  Fail\n",
      "Question 36+14   True 50   Guess 40   Fail\n",
      "Question 16+8    True 24   Guess 25   Fail\n",
      "Question 196+204 True 400  Guess 300  Fail\n",
      "Question 73+527  True 600  Guess 500  Fail\n",
      "Question 21+19   True 40   Guess 30   Fail\n",
      "Question 372+328 True 700  Guess 600  Fail\n",
      "Question 2+10    True 12   Guess 11   Fail\n",
      "Question 21+19   True 40   Guess 30   Fail\n",
      "Question 73+0    True 73   Guess 72   Fail\n",
      "Question 73+0    True 73   Guess 72   Fail\n",
      "Question 17+583  True 600  Guess 500  Fail\n",
      "Question 73+0    True 73   Guess 72   Fail\n",
      "Question 372+328 True 700  Guess 600  Fail\n",
      "Question 36+14   True 50   Guess 40   Fail\n",
      "Question 3+20    True 23   Guess 22   Fail\n",
      "Question 644+355 True 999  Guess 199  Fail\n",
      "Question 73+0    True 73   Guess 72   Fail\n",
      "Question 3+20    True 23   Guess 22   Fail\n",
      "Question 36+14   True 50   Guess 40   Fail\n",
      "Question 274+725 True 999  Guess 199  Fail\n",
      "Question 290+12  True 302  Guess 202  Fail\n",
      "Question 274+725 True 999  Guess 199  Fail\n",
      "Question 37+9    True 46   Guess 47   Fail\n",
      "Question 36+14   True 50   Guess 40   Fail\n",
      "Question 275+25  True 300  Guess 200  Fail\n",
      "Question 17+583  True 600  Guess 500  Fail\n",
      "Question 16+8    True 24   Guess 25   Fail\n",
      "Question 274+725 True 999  Guess 199  Fail\n",
      "Question 17+583  True 600  Guess 500  Fail\n",
      "Question 3+20    True 23   Guess 22   Fail\n",
      "The model scored  99.885  % in its test.\n"
     ]
    }
   ],
   "source": [
    "numtotal = 20000\n",
    "for i in range(numtotal):\n",
    "    ind = np.random.randint(0, len(x_val))\n",
    "    rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "    preds = model.predict_classes(rowx, verbose=0)\n",
    "    q = ctable.decode(rowx[0])\n",
    "    correct = ctable.decode(rowy[0])\n",
    "    guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "    if guess == correct :\n",
    "        numcorrect += 1\n",
    "    else:\n",
    "        print('Question', q, end=' ')\n",
    "        print('True', correct, end=' ')\n",
    "        print('Guess', guess, end=' ')\n",
    "        print('Fail')\n",
    "print('The model scored ', numcorrect*100/numtotal,' % in its test.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE\n",
    "\n",
    " * Try changing the hyperparams, use other RNNs, more layers, check if increasing the number of epochs is useful.\n",
    "\n",
    " * Try reversing the data from validation set and check if commutative property of addition is learned by the model.\n",
    " \n",
    " * Try printing the hidden layer with two inputs that are commutative and check if the hidden representations it learned are same or similar. Do we expect it to be true? If so, why? If not why? You can access the layer using an index with model.layers and layer.output will give the output of that layer.\n",
    " \n",
    " * Try doing addition in the RNN model the same way we do by hand. Reverse the order of digits and at each time step, input two digits get an output use the hidden layer and input next two digits and so on. (units in the first time step, tens in the second time step etc.)\n",
    " \n",
    "## Extra HW tidbits:\n",
    "\n",
    "### pad_sequences()\n",
    "When working with sequences, it is usually important to ensure that our input sequences are of the same length -- especially when doing reading inputs in batches. A simple and common approach is to make our input length be that of a our longest input. All inputs that are shorter than this will be padded with a particular value of your choice. Also, if any inputs happen to be longer than our specified length (again, we typically set this to be that of our longest input), then they will be truncated to that length. [Click here for the Keras documentation](https://keras.io/preprocessing/sequence/)\n",
    "\n",
    "### Embedding layers\n",
    "Embeddings layers, when used in Keras, must be the 1st layer of your network. They transform vectors from one space into another. Typically, this learned embedding is of a smaller size than the original. The power of this layer is that the Embeddings will be learned by the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

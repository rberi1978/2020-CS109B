{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "##  Project F: Measuring the Shape and Brightness of Galaxies with Neural Networks\n",
    "### Ideation 7\n",
    "### Group 75: Dmitry Vukolov, Ning Xu, Rohit Beri, Sunil Chomal\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Mark Glickman and Chris Tanner<br/>\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL\n",
    "import requests\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"theme\"> Outline </div> \n",
    "\n",
    "1. Overview\n",
    "2. Loading the Data\n",
    "3. Structure and Size of the Data\n",
    "4. Basic Statistics of the Data \n",
    "5. Visualization of the Image Data \n",
    "6. Distributions of the Labels and Numerical Attributes \n",
    "7. Parametric Image Generation\n",
    "8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"theme\"> Overview </div> \n",
    "\n",
    "<div style=\"border: 3px solid #800080; padding: 10px\"> \n",
    "\n",
    "## Creating Galaxy images using GalSim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"theme\"> Part 1 </div> \n",
    "\n",
    "<div style=\"border: 3px solid #800080; padding: 10px\"> \n",
    "\n",
    "## GalSim Images with fixed Sigma and PSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class='exercise'> <b> Step 0: Load the essential libraries </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load useful libraries\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Import h5py for reading h5 files\n",
    "import h5py\n",
    "\n",
    "# Load galsim for data generation\n",
    "import galsim\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tweak plot resolution and styling\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "sns.set(style=\"white\", palette=None, rc={\"axes.linewidth\": 1})\n",
    "plt.rc(\"image\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load useful libraries\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class='exercise'> <b> Step 1: Load the dataset with fixed noise and fixed PSF </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables: ['img', 'img_nonoise', 'label', 'psf_r', 'snr', 'sigma']\n"
     ]
    }
   ],
   "source": [
    "# Read the data set\n",
    "dataset = \"./cs109b-project/data/data_v3.npz\"\n",
    "\n",
    "with np.load(dataset) as data:\n",
    "    print(\"Available variables:\", data.files)\n",
    "\n",
    "    image = data[\"img\"]\n",
    "    image_nonoise = data[\"img_nonoise\"]\n",
    "    label = data[\"label\"]\n",
    "    snr = data[\"snr\"]\n",
    "    sigma = data[\"sigma\"]\n",
    "    psf = data[\"psf_r\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class='exercise'> <b> Step 2: Normalize the data and split the data into Traning and Validation sets </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images to 3D array\n",
    "image = image.reshape(image.shape[0], image.shape[1], image.shape[2], 1)\n",
    "image_nonoise = image_nonoise.reshape(image_nonoise.shape[0], image_nonoise.shape[1], image_nonoise.shape[2], 1)\n",
    "\n",
    "\n",
    "# Split into 90% train and 10% test (creates a view of the array)\n",
    "n_train = int(label.shape[0] * 0.9)\n",
    "image_train, image_val = image[:n_train], image[n_train:]\n",
    "image_nonoise_train, image_nonoise_val = image_nonoise[:n_train], image_nonoise[n_train:]\n",
    "label_train, label_val = label[:n_train], label[n_train:]\n",
    "snr_train, snr_val = snr[:n_train], snr[n_train:]\n",
    "sigma_train, sigma_val = sigma[:n_train], sigma[n_train:]\n",
    "psf_train, psf_val = psf[:n_train], psf[n_train:]\n",
    "\n",
    "\n",
    "# Zero-One Normalization of the labels\n",
    "label_min = label.min(axis=0)\n",
    "label_max = label.max(axis=0)\n",
    "label_diff = np.maximum((label_max - label_min),1e-10)\n",
    "\n",
    "label_train = (label_train - label_min)/label_diff\n",
    "label_val = (label_val - label_min)/label_diff\n",
    "\n",
    "\n",
    "# Zero-One Normalization of the no-noise images\n",
    "image_nonoise_min = image_nonoise.min()\n",
    "image_nonoise_max = image_nonoise.max()\n",
    "image_nonoise_diff = (image_nonoise_max - image_nonoise_min)\n",
    "\n",
    "image_nonoise_train = (image_nonoise_train - image_nonoise_min)/image_nonoise_diff\n",
    "image_nonoise_val = (image_nonoise_val - image_nonoise_min)/image_nonoise_diff\n",
    "\n",
    "\n",
    "# Zero-One Normalization of the noisy images\n",
    "image_min = image.min()\n",
    "image_max = image.max()\n",
    "image_diff = (image_max - image_min)\n",
    "\n",
    "image_train = (image_train - image_min)/image_diff\n",
    "image_val = (image_val - image_min)/image_diff\n",
    "\n",
    "\n",
    "# Compute Image Statistics\n",
    "stats = np.hstack([image.mean(axis=(1,2)).reshape(-1,1), image.std(axis=(1,2)).reshape(-1,1), \n",
    "                   image.min(axis=(1,2)).reshape(-1,1), image.max(axis=(1,2)).reshape(-1,1)])\n",
    "stats_train, stats_val = stats[:n_train], stats[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class='exercise'> <b> Step 3: Structure and Size of the Data </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape            = (200000, 64, 64, 1)\n",
      "No-noise Image Shape   = (200000, 64, 64, 1)\n",
      "Label Shape            = (200000, 5)\n",
      "SNR Shape              = (200000,)\n",
      "Sigma Shape            = (200000,)\n",
      "PSF-R Shape            = (200000,)\n",
      "Stats Shape            = (200000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Structure and Size of the Data\n",
    "print(\"Image Shape            = {}\".format(image.shape))\n",
    "print(\"No-noise Image Shape   = {}\".format(image_nonoise.shape))\n",
    "print(\"Label Shape            = {}\".format(label.shape))\n",
    "print(\"SNR Shape              = {}\".format(snr.shape))\n",
    "print(\"Sigma Shape            = {}\".format(sigma.shape))\n",
    "print(\"PSF-R Shape            = {}\".format(psf.shape))\n",
    "print(\"Stats Shape            = {}\".format(stats.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class='exercise'> <b> Step 4: Create Data Pipeline </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Create Tensorflow Dataset </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow Dataset\n",
    "training = tf.data.Dataset.from_tensor_slices({\n",
    "    \"Image\": image_train,\n",
    "    \"No-noise Image\": image_nonoise_train,\n",
    "    \"Labels\": label_train,\n",
    "    \"Stats\": stats_train\n",
    "})\n",
    "\n",
    "validation = tf.data.Dataset.from_tensor_slices({\n",
    "    \"Image\": image_val,\n",
    "    \"No-noise Image\": image_nonoise_val,\n",
    "    \"Labels\": label_val,\n",
    "    \"Stats\": stats_val\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Define pipeline functions to structure dataset </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_clean_image_to_label(element):\n",
    "    image = element['Image']\n",
    "    label = tf.cast(element['Labels'], tf.float32)\n",
    "    \n",
    "    return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_noisy_image_to_clean_image(element):\n",
    "    image = element['Image']\n",
    "    stats = tf.cast(element['Stats'], tf.float32)\n",
    "    clean = element['No-noise Image']\n",
    "    \n",
    "    return ((image, stats), clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_noisy_image_to_label(element):\n",
    "    image = element['Image']\n",
    "    stats = tf.cast(element['Stats'], tf.float32)\n",
    "    label = tf.cast(element['Labels'], tf.float32)\n",
    "    \n",
    "    return ((image, stats), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Create dataset pipeline </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "# Data pipeline for Reverse Encoder\n",
    "tr_ds_en = training.map(\n",
    "    pipeline_clean_image_to_label).shuffle(1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_ds_en = validation.map(\n",
    "    pipeline_clean_image_to_label).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Data pipeline for Conditional Autoencoder\n",
    "tr_ds_AE = training.map(\n",
    "    pipeline_noisy_image_to_clean_image).shuffle(1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_ds_AE = validation.map(\n",
    "    pipeline_noisy_image_to_clean_image).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Data pipeline for Triple Conditional Autoencoder\n",
    "tr_ds_tCAE = training.map(\n",
    "    pipeline_noisy_image_to_label).shuffle(1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_ds_tCAE = validation.map(\n",
    "    pipeline_noisy_image_to_label).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class='exercise'> <b> Step 5: Create Convolution Network </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Declare global model parameters </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Encoder Inout Dimension\n",
    "input_shape_img = (64,64,1)\n",
    "input_shape_stats = (4,)\n",
    "\n",
    "# Dropout rate for dense layers\n",
    "dropout = 0\n",
    "\n",
    "# Encoder Output dimension - Decoder Input Dimension\n",
    "latent_z_dim_labels = 5\n",
    "\n",
    "# Decoder latent dimension\n",
    "latent_dim = (1,1,1024)\n",
    "\n",
    "# Number of Training Epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Define convolution encoder </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Convolutional Encoder Layer Class.\n",
    "    Converts an input into a latent representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, dropout_rate=0.0, name='encoder', **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the encoder layers and saves them as local attribute.\n",
    "        \n",
    "        Input:\n",
    "        -input_dim: 3D-tuple with (rows, cols, channels) input image dimensions.\n",
    "        \n",
    "        Returns nothing.\n",
    "        \"\"\"\n",
    "        super(ConvEncoder, self).__init__(name=name, input_shape=input_shape, **kwargs)\n",
    "        \n",
    "        ## your code here\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', strides=(2,2))\n",
    "        self.lRelu1 = layers.LeakyReLU()\n",
    "        self.drop1 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', strides=(2,2))\n",
    "        self.lRelu2 = layers.LeakyReLU()\n",
    "        self.drop2 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(filters=128, kernel_size=(3,3), padding='same', strides=(2,2))\n",
    "        self.lRelu3 = layers.LeakyReLU()\n",
    "        self.drop3 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv4 = layers.Conv2D(filters=256, kernel_size=(3,3), padding='same', strides=(2,2))\n",
    "        self.lRelu4 = layers.LeakyReLU()\n",
    "        self.drop4 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv5 = layers.Conv2D(filters=512, kernel_size=(3,3), padding='same', strides=(2,2))\n",
    "        self.lRelu5 = layers.LeakyReLU()\n",
    "        self.drop5 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv6 = layers.Conv2D(filters=1024, kernel_size=(3,3), padding='same', strides=(2,2))\n",
    "        self.lRelu6 = layers.LeakyReLU()\n",
    "        self.drop6 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # end of your code here\n",
    "        \n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Runs the encoding inference for `inputs`.\n",
    "        \n",
    "        Inputs:\n",
    "        -inputs: 4D-tensor with dimension (batch_size, self.input_dim).\n",
    "        \"\"\"\n",
    "        ## your code here\n",
    "        \n",
    "        z = self.conv1(inputs)\n",
    "        z = self.lRelu1(z)\n",
    "        z = self.drop1(z)\n",
    "        \n",
    "        z = self.conv2(z)\n",
    "        z = self.lRelu2(z)\n",
    "        z = self.drop2(z)\n",
    "        \n",
    "        z = self.conv3(z)\n",
    "        z = self.lRelu3(z)\n",
    "        z = self.drop3(z)\n",
    "        \n",
    "        z = self.conv4(z)\n",
    "        z = self.lRelu4(z)\n",
    "        z = self.drop4(z)\n",
    "        \n",
    "        z = self.conv5(z)\n",
    "        z = self.lRelu5(z)\n",
    "        z = self.drop5(z)\n",
    "        \n",
    "        z = self.conv6(z)\n",
    "        z = self.lRelu6(z)\n",
    "        z = self.drop6(z)\n",
    "        \n",
    "        # end of your code here\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Create Noisy Image to Label CNN </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Reverse_Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (ConvEncoder)        (None, 1, 1, 1024)        6287616   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              4198400   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 21,632,261\n",
      "Trainable params: 21,632,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create CNN Model\n",
    "def create_reverse_encoder():\n",
    "\n",
    "    # Create Reverse Encoder\n",
    "    input_img = layers.Input(input_shape_img)\n",
    "    \n",
    "    x = ConvEncoder(input_shape_img)(input_img)\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    x = layers.Dense(4096)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    x = layers.Dense(2048)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    labels = layers.Dense(latent_z_dim_labels, activation='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    # Encoder Model\n",
    "    reverse_encoder = tf.keras.Model(input_img, labels, name='Reverse_Encoder')\n",
    "    display(reverse_encoder.summary())\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "    reverse_encoder.compile(optimizer, loss=losses.BinaryCrossentropy())\n",
    "    \n",
    "    # Return reverse decoder\n",
    "    return reverse_encoder\n",
    "\n",
    "\n",
    "reverse_encoder = create_reverse_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Traning reverse encoder</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 19s 26ms/step - loss: 0.5891\n",
      "Training Loss:   0.58914\n",
      "79/79 [==============================] - 2s 24ms/step - loss: 0.5919\n",
      "Validation Loss: 0.59188\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "try:\n",
    "    reverse_encoder.load_weights('./model/rev_en7')\n",
    "    print(\"Training Loss:   {:0.5f}\".format(reverse_encoder.evaluate(tr_ds_en)))\n",
    "    print(\"Validation Loss: {:0.5f}\".format(reverse_encoder.evaluate(val_ds_en)))\n",
    "    \n",
    "except:\n",
    "    history = reverse_encoder.fit(\n",
    "        tr_ds_en, \n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        validation_data=val_ds_en,\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    "    )\n",
    "    \n",
    "    reverse_encoder.save_weights('./model/rev_en7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='gc'> <b> Variance Analysis </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:                  (11011.742, 1.1049073, 0.094649605, 0.11115542, 0.11500337)\n",
      "Unexplained Variance:  (0.013728012, 0.48152396, 0.4444273, 0.16382723, 0.17472029)\n",
      "Explained Variance:    (0.986272, 0.518476, 0.5555727, 0.83617276, 0.8252797)\n"
     ]
    }
   ],
   "source": [
    "pred = reverse_encoder.predict(val_ds_en)\n",
    "error_unscaled = (pred - label_val) * label_diff\n",
    "\n",
    "MSE = (error_unscaled**2).mean(axis=0)\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "label_var = (label_val.std(axis=0) * label_diff)**2\n",
    "\n",
    "unexplained_var = MSE/np.maximum(label_var, 1e-10)\n",
    "\n",
    "print(\"RMSE:                  {}\".format(tuple(RMSE)))\n",
    "print(\"Unexplained Variance:  {}\".format(tuple(unexplained_var)))\n",
    "print(\"Explained Variance:    {}\".format(tuple(1-unexplained_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"theme\"> End of Part 1 </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"theme\"> Summary </div> \n",
    "\n",
    "<div style=\"border: 3px solid #800080; padding: 10px\"> \n",
    "    \n",
    "### ***Shape and Brightness of a Galaxy***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\">  End Of Project\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
